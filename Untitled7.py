{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMus0QBorLB4dag2MoEDRX9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d0f6a696d1794589bf520648b752cef3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d1a04713d0b4419ba561c1d2b05e337d","IPY_MODEL_49fa48283fed4c588671722159fc22b4","IPY_MODEL_8a0eebadc25b40cb9238b187bdc553d2"],"layout":"IPY_MODEL_81ba05e8efe84b6a8308a27227a1b89a"}},"d1a04713d0b4419ba561c1d2b05e337d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3aca491c87a54bbc87d44f9974ec589f","placeholder":"​","style":"IPY_MODEL_00ff695470b6411ca93cb12baa46521e","value":"Original Training:  50%"}},"49fa48283fed4c588671722159fc22b4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab8da2681e124f84a0e30ce44f321009","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4415383e7cfa4e3e80bf8c62b0c69726","value":5}},"8a0eebadc25b40cb9238b187bdc553d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf8473a1b88441678d989f8dacbf6ffd","placeholder":"​","style":"IPY_MODEL_aa967eb6b571469b98d47cb903e3a9d2","value":" 5/10 [16:19&lt;12:37, 151.52s/it, RMSE=0.72, loss=0.5494]"}},"81ba05e8efe84b6a8308a27227a1b89a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3aca491c87a54bbc87d44f9974ec589f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00ff695470b6411ca93cb12baa46521e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab8da2681e124f84a0e30ce44f321009":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4415383e7cfa4e3e80bf8c62b0c69726":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cf8473a1b88441678d989f8dacbf6ffd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa967eb6b571469b98d47cb903e3a9d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":327,"referenced_widgets":["d0f6a696d1794589bf520648b752cef3","d1a04713d0b4419ba561c1d2b05e337d","49fa48283fed4c588671722159fc22b4","8a0eebadc25b40cb9238b187bdc553d2","81ba05e8efe84b6a8308a27227a1b89a","3aca491c87a54bbc87d44f9974ec589f","00ff695470b6411ca93cb12baa46521e","ab8da2681e124f84a0e30ce44f321009","4415383e7cfa4e3e80bf8c62b0c69726","cf8473a1b88441678d989f8dacbf6ffd","aa967eb6b571469b98d47cb903e3a9d2"]},"id":"Ppo5V-Ycp13o","outputId":"0766801b-ff6c-435e-e729-e63175cfba46"},"outputs":[{"output_type":"stream","name":"stdout","text":["LSTM model input attributes ['H_bucket', 'rA_spigot', 'rH_spigot', 'soil_depth']\n","Using CPU\n","Progress: 20% complete.\n","Progress: 40% complete.\n","Progress: 60% complete.\n","Progress: 80% complete.\n","Progress: 100% complete.\n","\n","============================================================\n","TRAINING ORIGINAL LSTM MODEL\n","============================================================\n"]},{"output_type":"display_data","data":{"text/plain":["Original Training:   0%|          | 0/10 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0f6a696d1794589bf520648b752cef3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 0 - Loss: 0.8946, RMSE: 0.9088\n","Epoch 1 - Loss: 0.7927, RMSE: 0.8627\n","Epoch 2 - Loss: 0.6689, RMSE: 0.7805\n","Epoch 3 - Loss: 0.6006, RMSE: 0.7487\n","Epoch 4 - Loss: 0.5494, RMSE: 0.7185\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import scipy.stats as stats\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data as data_utils\n","from torch.autograd import Variable\n","import sklearn\n","from sklearn.preprocessing import StandardScaler\n","from tqdm.notebook import trange, tqdm\n","import os\n","\n","from pyflo import system\n","from pyflo.nrcs import hydrology\n","uh484 = system.array_from_csv('distributions/scs484.csv')\n","\n","g = 1.271e8\n","time_step = 1\n","rain_probability_range = {\"None\": [0.3, 0.4],\n","                          \"Light\": [0.4, 0.5],\n","                          \"Heavy\": [0.1, 0.3]}\n","\n","threshold_precip = 0.01 # precip value boundary between \"light\" and \"heavy\"\n","max_precip = 0.25 # max amount of precip possible\n","\n","# distribution parameters\n","rain_depth_range = {\"Light\": [0.0008108, 0.0009759], \"Heavy\": [0.2341, 0.0101, 0.009250]}\n","bucket_attributes_range = {\"A_bucket\": [5e2, 2e3],\n","                           \"H_bucket\": [0.1, 0.3],\n","                           \"rA_spigot\": [0.1, 0.2], # calculations to be a function of H_bucket\n","                           \"rH_spigot\": [0.05, 0.15], # calculations to be a function of H_bucket\n","                           ### The following two parameters come from standard distributions based on real data.\n","                           # Do not change these:\n","                           \"K_infiltration\": [-13.8857, 1.1835], # location and scale of normal distribution\n","                           \"ET_parameter\": [2.2447, 9.9807e-5, 0.0016], # shape, loc, and scale of Weibull min dist\n","\n","                           \"soil_depth\": [0.3, 0.8]\n","                          }\n","bucket_attributes_list = list(bucket_attributes_range.keys())\n","bucket_attributes_list.append('A_spigot')\n","bucket_attributes_list.append('H_spigot')\n","bucket_attributes_lstm_inputs = ['H_bucket', 'rA_spigot', 'rH_spigot', 'soil_depth']\n","print(\"LSTM model input attributes\", bucket_attributes_lstm_inputs)\n","input_vars = ['precip', 'et', 'h_bucket']\n","input_vars.extend(bucket_attributes_lstm_inputs)\n","output_vars = ['q_total', 'q_overflow', 'q_spigot']\n","n_input = len(input_vars)\n","n_output = len(output_vars)\n","\n","noise = {\"pet\": 0.1, \"et\": 0.1, \"q\": 0.1, \"head\": 0.1}\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda:0\")\n","    print(\"Using CUDA device:\", torch.cuda.get_device_name(0))\n","elif torch.backends.mps.is_available():\n","    device = torch.device(\"mps\")\n","    print(\"Using Apple M3/M2/M1 (Metal) device\")\n","else:\n","    device = 'cpu'\n","    print(\"Using CPU\")\n","hidden_state_size = 128\n","num_layers = 1\n","num_epochs = 10\n","batch_size = 128\n","seq_length = 336\n","learning_rate = np.linspace(start=0.001, stop=0.0001, num=num_epochs)\n","\n","n_buckets_split = {\"train\": 20, \"val\": 10,\"test\": 1}\n","time_splits = {\"warmup\":256, \"train\": 1032, \"val\": 1032,\"test\": 1032}\n","\n","\n","num_records = time_splits[\"warmup\"] + time_splits[\"train\"] + time_splits[\"val\"] + time_splits[\"test\"] + seq_length * 3\n","n_buckets = n_buckets_split[\"train\"] + n_buckets_split[\"val\"] + n_buckets_split[\"test\"]\n","\n","def split_parameters():\n","    # create lists of bucket indices for each set based on the given bucket splits\n","    buckets_for_training = list(range(0, n_buckets_split['train'] + 1))\n","    buckets_for_val = list(range(n_buckets_split['train'] + 1,\n","                                 n_buckets_split['train'] + n_buckets_split['val'] + 1))\n","    buckets_for_test = list(range(n_buckets - n_buckets_split['test'], n_buckets))\n","\n","    # determine the time range for each set based on the given time splits\n","    train_start = time_splits[\"warmup\"] + seq_length\n","    train_end   = time_splits[\"warmup\"] + time_splits[\"train\"]\n","    val_start   = train_end + seq_length\n","    val_end     = val_start + time_splits[\"val\"]\n","    test_start  = val_end + seq_length\n","    test_end    = test_start + time_splits[\"test\"]\n","\n","    # organize the split parameters into separate lists for each set\n","    train_split_parameters = [buckets_for_training, train_start, train_end]\n","    val_split_parameters = [buckets_for_val, val_start, val_end]\n","    test_split_parameters = [buckets_for_test, test_start, test_end]\n","\n","    return [train_split_parameters, val_split_parameters, test_split_parameters]\n","\n","[[buckets_for_training, train_start, train_end],\n","[buckets_for_val, val_start, val_end],\n","[buckets_for_test, test_start, test_end]]= split_parameters()\n","\n","def setup_buckets(n_buckets):\n","    # Boundary conditions\n","    buckets = {bucket_attribute:[] for bucket_attribute in bucket_attributes_list}\n","    buckets['A_spigot'] = []\n","    buckets['H_spigot'] = []\n","    for i in range(n_buckets):\n","        for attribute in bucket_attributes_list:\n","            if attribute == 'A_bucket' or attribute == 'H_bucket' or attribute == 'rA_spigot' or attribute == 'rH_spigot' or attribute == 'soil_depth':\n","                buckets[attribute].append(np.random.uniform(bucket_attributes_range[attribute][0],\n","                                                        bucket_attributes_range[attribute][1]))\n","            if attribute == 'K_infiltration':\n","                buckets[attribute].append(np.random.normal(bucket_attributes_range[attribute][0],\n","                                                        bucket_attributes_range[attribute][1]))\n","\n","            if attribute == \"ET_parameter\":\n","                buckets[attribute].append(stats.weibull_min.rvs(bucket_attributes_range[attribute][0],\n","                                                                bucket_attributes_range[attribute][1],\n","                                                                bucket_attributes_range[attribute][2]))\n","\n","        buckets['A_spigot'].append(np.pi * (0.5 * buckets['H_bucket'][i] * buckets['rA_spigot'][i]) ** 2)\n","        buckets['H_spigot'].append(buckets['H_bucket'][i] * buckets['rH_spigot'][i])\n","\n","    # Initial conditions\n","    h_water_level = [np.random.uniform(0, buckets[\"H_bucket\"][i]) for i in range(n_buckets)]\n","    mass_overflow = [0]*n_buckets\n","\n","    return buckets, h_water_level, mass_overflow\n","\n","buckets, h_water_level, mass_overflow = setup_buckets(n_buckets)\n","\n","def pick_rain_params():\n","    buck_rain_params = [rain_depth_range,\n","                        np.random.uniform(rain_probability_range[\"None\"][0],\n","                                            rain_probability_range[\"None\"][1]),\n","                        np.random.uniform(rain_probability_range[\"Heavy\"][0],\n","                                            rain_probability_range[\"Heavy\"][1]),\n","                        np.random.uniform(rain_probability_range[\"Light\"][0],\n","                                            rain_probability_range[\"Light\"][1])\n","                 ]\n","    return buck_rain_params\n","\n","def random_rain(preceding_rain, bucket_rain_params):\n","    depth_range, no_rain_probability, light_rain_probability, heavy_rain_probability = bucket_rain_params\n","    # some percent of time we have no rain at all\n","    if np.random.uniform(0.01, 0.99) < no_rain_probability:\n","        rain = 0\n","\n","    # When we do have rain, the probability of heavy or light rain depends on the previous hour's rainfall\n","    else:\n","        rain = np.inf\n","        # If last hour was a light rainy hour, or no rain, then we are likely to have light rain this hour\n","        if preceding_rain < threshold_precip:\n","            if np.random.uniform(0.0, 1.0) < light_rain_probability:\n","                while rain < 0 or rain > threshold_precip:\n","                    rain = stats.gumbel_r.rvs(depth_range[\"Light\"][0], depth_range[\"Light\"][1])\n","            else:\n","                # But if we do have heavy rain, then it could be very heavy\n","                while rain < threshold_precip or rain > max_precip:\n","                    rain = stats.genpareto.rvs(depth_range[\"Heavy\"][0], depth_range[\"Heavy\"][1], depth_range[\"Heavy\"][2])\n","\n","        # If it was heavy rain last hour, then we might have heavy rain again this hour\n","        else:\n","            if np.random.uniform(0.0, 1.0) < heavy_rain_probability:\n","                while rain < threshold_precip or rain > max_precip:\n","                    rain = stats.genpareto.rvs(depth_range[\"Heavy\"][0], depth_range[\"Heavy\"][1], depth_range[\"Heavy\"][2])\n","            else:\n","                while rain < 0 or rain > threshold_precip:\n","                    rain = stats.gumbel_r.rvs(depth_range[\"Light\"][0], depth_range[\"Light\"][1])\n","    return rain\n","\n","in_list = {}\n","for ibuc in range(n_buckets):\n","    bucket_rain_params = pick_rain_params()\n","    in_list[ibuc] = [0]\n","    for i in range(1, num_records):\n","        in_list[ibuc].append(random_rain(in_list[ibuc][i-1], bucket_rain_params))\n","\n","def apply_unit_hydrograph(df, ibuc):\n","    \"\"\"Given a bucket‐simulation DataFrame with 'q_overflow' and 'q_spigot' (both in m/s normalized by area),\n","    compute and append a 'q_total' column by routing combined runoff through a unit hydrograph.\n","    \"\"\"\n","    # build the Basin object\n","    area_acres = buckets[\"A_bucket\"][ibuc] / 4047\n","    basin = hydrology.Basin(\n","        area       = area_acres,\n","        cn         = 83.0,\n","        tc         = 2.3,\n","        runoff_dist= uh484,\n","        peak_factor= 1\n","    )\n","\n","    # prepare cumulative‐inch input\n","    n = len(df)\n","    q_in = np.zeros((n, 2))\n","    cum_inches = 0.0\n","    for i in range(n):\n","        cum_inches += (df.loc[i,'q_overflow'] + df.loc[i,'q_spigot']) * 39.3701\n","        q_in[i] = (i, cum_inches)\n","\n","    # run UH\n","    full = basin.flood_hydrograph(q_in, interval=1)[:,1]\n","\n","    # trim or pad to match df length\n","    if len(full) >= n:\n","        out = full[:n]\n","    else:\n","        out = np.pad(full, (0, n-len(full)), 'constant')\n","\n","    # convert back to m per time step, normalized by area\n","    df['q_total'] = out / 35.315 / buckets[\"A_bucket\"][ibuc] * 3600\n","\n","    return df\n","\n","def run_bucket_simulation(ibuc):\n","    columns = ['precip', 'et', 'infiltration', 'h_bucket', 'q_overflow', 'q_spigot', 'q_total']\n","    columns.extend(bucket_attributes_list)\n","    # Memory to store model results\n","    df = pd.DataFrame(index=list(range(len(in_list[ibuc]))), columns=columns)\n","\n","    # Main loop through time\n","    for t, precip_in in enumerate(in_list[ibuc]):\n","\n","        # Add the input mass to the bucket\n","        h_water_level[ibuc] = h_water_level[ibuc] + precip_in\n","\n","        # Lose mass out of the bucket. Some periodic type loss, evaporation, and some infiltration...\n","\n","        # ET (m/s) is the value at each time step taking diurnal fluctuations into account. The definite integral of the following function\n","        # (excluding noise) from 0 to 24 is equal to ET_parameter, which is measured in m/day.\n","        et = np.max([0, ((1/7.6394)* buckets[\"ET_parameter\"][ibuc]) * np.sin((np.pi / 12)*t) * np.random.normal(1, noise['pet'])])\n","\n","        k = 10 ** buckets['K_infiltration'][ibuc]\n","        L = buckets['soil_depth'][ibuc]\n","\n","        # Calculate infiltration using Darcy's Law: Q = (k * ρ * g * A * Δh) / (μ * L) → infiltration = Q / A\n","        # Final form: infiltration = (k * ρ * g * Δh) / (μ * L), with Δh = soil depth + water level height\n","        delta_h = h_water_level[ibuc] + L\n","        infiltration = k * delta_h / L\n","\n","        h_water_level[ibuc] = np.max([0 , (h_water_level[ibuc] - et)])\n","        h_water_level[ibuc] = np.max([0 , (h_water_level[ibuc] - infiltration)])\n","        h_water_level[ibuc] = h_water_level[ibuc] * np.random.normal(1, noise['et'])\n","\n","        # Overflow if the bucket is too full\n","        if h_water_level[ibuc] > buckets[\"H_bucket\"][ibuc]:\n","            mass_overflow[ibuc] = h_water_level[ibuc] - buckets[\"H_bucket\"][ibuc]\n","            h_water_level[ibuc] = buckets[\"H_bucket\"][ibuc]\n","            h_water_level[ibuc] = h_water_level[ibuc] - np.random.uniform(0, noise['q'])\n","\n","        # Calculate head on the spigot\n","        h_head_over_spigot = (h_water_level[ibuc] - buckets[\"H_spigot\"][ibuc] )\n","        h_head_over_spigot = h_head_over_spigot * np.random.normal(1, noise['head'])\n","\n","        # Calculate water leaving bucket through spigot\n","        if h_head_over_spigot > 0:\n","            velocity_out = np.sqrt(2 * g * h_head_over_spigot)\n","            spigot_out_volume = velocity_out *  buckets[\"A_spigot\"][ibuc] * time_step\n","\n","            # prevents spigot from draining water below H_spigot\n","            spigot_out = np.min([spigot_out_volume / buckets[\"A_bucket\"][ibuc], h_head_over_spigot])\n","            h_water_level[ibuc] -= spigot_out\n","        else:\n","            spigot_out = 0\n","\n","        # Save the data in time series\n","        df.loc[t,'precip'] = precip_in\n","        df.loc[t,'et'] = et\n","        df.loc[t,'infiltration'] = infiltration\n","        df.loc[t,'h_bucket'] = h_water_level[ibuc]\n","        df.loc[t,'q_overflow'] = mass_overflow[ibuc]\n","        df.loc[t,'q_spigot'] = spigot_out\n","        for attribute in bucket_attributes_list:\n","            df.loc[t, attribute] = buckets[attribute][ibuc]\n","\n","        mass_overflow[ibuc] = 0\n","\n","    # --- route through unit hydrograph ---\n","    df = apply_unit_hydrograph(df, ibuc)\n","\n","    # ---- mass tracking columns ----\n","    # all in meters of water per time step, per unit area\n","    df['cum_precip']   = df['precip'].cumsum()\n","    df['cum_et']       = df['et'].cumsum()\n","    df['cum_inf']      = df['infiltration'].cumsum()\n","    df['cum_runoff']   = df['q_overflow'].cumsum() + df['q_spigot'].cumsum()\n","    df['storage']      = df['h_bucket']\n","    df['mass_out_tot'] = df['cum_et'] + df['cum_inf'] + df['cum_runoff'] + df['storage']\n","    df['residual_frac']= (df['cum_precip'] - df['mass_out_tot']) / df['cum_precip']\n","    # --------------------------------\n","\n","    return df\n","\n","bucket_dictionary = {}\n","\n","# Define the progress milestones\n","milestones = [0.2, 0.4, 0.6, 0.8, 1.0]\n","n_buckets_completed = 0  # Counter for completed buckets\n","\n","for ibuc in range(n_buckets):\n","    bucket_dictionary[ibuc] = run_bucket_simulation(ibuc)\n","\n","    # Increment the completed bucket counter\n","    n_buckets_completed += 1\n","\n","    # Calculate the current progress as a fraction\n","    progress = n_buckets_completed / n_buckets\n","\n","    # Check if we have reached any of the milestones\n","    for milestone in milestones:\n","        if progress >= milestone:\n","            print(f\"Progress: {int(milestone * 100)}% complete.\")\n","            milestones.remove(milestone)  # Remove the milestone once it is reached\n","            break  # To avoid printing multiple milestones at once\n","\n","# =========================================\n","# MODIFIED LSTM CLASS WITH PERSISTENT STATE\n","# =========================================\n","class PersistentLSTM(nn.Module):\n","    def __init__(self, num_classes, input_size, hidden_size, num_layers, batch_size, seq_length):\n","        super(PersistentLSTM, self).__init__()\n","        self.num_classes = num_classes\n","        self.num_layers = num_layers\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.seq_length = seq_length\n","        self.batch_size = batch_size\n","\n","        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n","                           num_layers=num_layers, batch_first=True)\n","        self.relu = nn.ReLU()\n","        self.fc_1 = nn.Linear(hidden_size, num_classes)\n","        self.hidden = None\n","\n","    def forward(self, x, init_states=None):\n","        if init_states is not None:\n","            self.hidden = init_states\n","\n","        out, self.hidden = self.lstm(x, self.hidden)\n","        out = self.relu(out)\n","        prediction = self.fc_1(out)\n","        prediction = self.relu(prediction)\n","        return prediction\n","\n","    def init_hidden(self, batch_size=1, device='cpu'):\n","        self.hidden = (\n","            torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n","            torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n","        )\n","\n","    def detach_hidden(self):\n","        if self.hidden is not None:\n","            self.hidden = tuple(h.detach() for h in self.hidden)\n","\n","    def save_hidden(self, filename):\n","        if self.hidden is not None:\n","            torch.save(self.hidden, filename)\n","\n","    def load_hidden(self, filename):\n","        if os.path.exists(filename):\n","            self.hidden = torch.load(filename, map_location=device)\n","        else:\n","            self.init_hidden(device=device)\n","\n","# Original LSTM class (for comparison)\n","class LSTM1(nn.Module):\n","    def __init__(self, num_classes, input_size, hidden_size, num_layers, batch_size, seq_length):\n","        super(LSTM1, self).__init__()\n","        self.num_classes = num_classes\n","        self.num_layers = num_layers\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.seq_length = seq_length\n","        self.batch_size = batch_size\n","\n","        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n","        self.relu = nn.ReLU()\n","        self.fc_1 = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x, init_states=None):\n","        if init_states is None:\n","            h_t = Variable(torch.zeros(1, x.size(0), self.hidden_size, device=x.device))\n","            c_t = Variable(torch.zeros(1, x.size(0), self.hidden_size, device=x.device))\n","            init_states = (h_t, c_t)\n","\n","        out, _ = self.lstm(x, init_states)\n","        out = self.relu(out)\n","        prediction = self.fc_1(out)\n","        prediction = self.relu(prediction)\n","        return prediction\n","\n","# =========================================\n","# MODIFIED TRAINING FUNCTIONS\n","# =========================================\n","def train_original_model(lstm, train_loader, buckets_for_training):\n","    \"\"\"Original training approach with state reset each batch\"\"\"\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(lstm.parameters(), lr=learning_rate[0])\n","    epoch_bar = tqdm(range(num_epochs), desc=\"Original Training\", position=0, total=num_epochs)\n","    results = {ibuc: {\"loss\": [], \"RMSE\": []} for ibuc in buckets_for_training}\n","\n","    for epoch in epoch_bar:\n","        epoch_loss = 0\n","        epoch_rmse = 0\n","        batch_count = 0\n","\n","        for ibuc in buckets_for_training:\n","            batch_bar = tqdm(\n","                train_loader[ibuc],\n","                desc=f\"Bucket: {ibuc}, Epoch: {epoch}\",\n","                position=1, leave=False, disable=True\n","            )\n","\n","            for data, targets in batch_bar:\n","                data, targets = data.to(device), targets.to(device)\n","                optimizer.zero_grad()\n","\n","                out = lstm(data)\n","                preds = out[:, -1:, :]  # last timestep\n","                true = targets[:, -1:, :]\n","                loss = criterion(preds, true)\n","                loss.backward()\n","                optimizer.step()\n","\n","                epoch_loss += loss.item()\n","                epoch_rmse += loss.sqrt().item()\n","                batch_count += 1\n","\n","                batch_bar.set_postfix(\n","                    loss=f\"{loss.item():.4f}\",\n","                    RMSE=f\"{loss.sqrt().item():.2f}\"\n","                )\n","\n","        avg_loss = epoch_loss / batch_count\n","        avg_rmse = epoch_rmse / batch_count\n","\n","        # Store results for all buckets (simplified)\n","        for ibuc in buckets_for_training:\n","            results[ibuc][\"loss\"].append(avg_loss)\n","            results[ibuc][\"RMSE\"].append(avg_rmse)\n","\n","        epoch_bar.set_postfix(\n","            loss=f\"{avg_loss:.4f}\",\n","            RMSE=f\"{avg_rmse:.2f}\"\n","        )\n","        print(f\"Epoch {epoch} - Loss: {avg_loss:.4f}, RMSE: {avg_rmse:.4f}\")\n","\n","    return lstm, results\n","\n","\n","\n","def train_persistent_model(model, train_loader, buckets_for_training):\n","    \"\"\"TRUE persistent training - state continues across ALL batches and ALL buckets\"\"\"\n","    model.to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate[0])\n","    criterion = nn.MSELoss()\n","\n","    epoch_bar = tqdm(range(num_epochs), desc=\"True Persistent Training\", position=0, total=num_epochs)\n","    results = {ibuc: {\"loss\": [], \"RMSE\": []} for ibuc in buckets_for_training}\n","\n","    for epoch in range(num_epochs):\n","        #  Initialize hidden state ONLY ONCE at start of epoch\n","        model.init_hidden(batch_size=batch_size, device=device)\n","\n","        epoch_loss = 0\n","        epoch_rmse = 0\n","        batch_count = 0\n","\n","        # Process ALL buckets in sequence with CONTINUOUS state\n","        for ibuc in buckets_for_training:\n","            for data, targets in train_loader[ibuc]:\n","                data, targets = data.to(device), targets.to(device)\n","                optimizer.zero_grad()\n","\n","                #  Hidden state automatically carries over from previous batch!\n","                out = model(data)\n","\n","                preds = out[:, -1:, :]\n","                true = targets[:, -1:, :]\n","                loss = criterion(preds, true)\n","                loss.backward()\n","                optimizer.step()\n","\n","                epoch_loss += loss.item()\n","                epoch_rmse += loss.sqrt().item()\n","                batch_count += 1\n","\n","                #  Detach to break computation graph, but PRESERVE state for next batch\n","                model.detach_hidden()\n","\n","        avg_loss = epoch_loss / batch_count\n","        avg_rmse = epoch_rmse / batch_count\n","\n","        for ibuc in buckets_for_training:\n","            results[ibuc][\"loss\"].append(avg_loss)\n","            results[ibuc][\"RMSE\"].append(avg_rmse)\n","\n","        epoch_bar.set_postfix(\n","            loss=f\"{avg_loss:.4f}\",\n","            RMSE=f\"{avg_rmse:.2f}\"\n","        )\n","        print(f\"Epoch {epoch} - Loss: {avg_loss:.4f}, RMSE: {avg_rmse:.4f}\")\n","\n","    return model, results\n","\n","# =========================================\n","# MODIFIED EVALUATION FUNCTIONS\n","# =========================================\n","def check_validation_period_comparison(lstm_original, lstm_persistent, np_val_seq_X, ibuc, n_plot=100):\n","    \"\"\"Compare both models on validation period\"\"\"\n","\n","    def __make_prediction(lstm_model, model_type=\"original\"):\n","        if model_type == \"persistent\":\n","            lstm_model.init_hidden(batch_size=1, device=device)\n","\n","        lstm_output_val = lstm_model(torch.Tensor(np_val_seq_X[ibuc]).to(device=device))\n","        val_predictions = {var: [] for var in output_vars}\n","\n","        for i in range(lstm_output_val.shape[0]):\n","            for j, var in enumerate(output_vars):\n","                val_predictions[var].append((lstm_output_val[i, -1, j].cpu().detach().numpy() * \\\n","                                             np.std(df.loc[train_start:train_end, var])) + \\\n","                                            np.mean(df.loc[train_start:train_end, var]))\n","        return val_predictions\n","\n","    def __compute_nse(val_predictions):\n","        nse_values = {}\n","        for var in output_vars:\n","            actual_values = df.loc[val_start:val_end, var]\n","            mean_actual = np.mean(actual_values)\n","            pred_variance = 0\n","            obs_variance = 0\n","\n","            for i, pred in enumerate(val_predictions[var]):\n","                t = i + seq_length - 1\n","                pred_variance += np.power((pred - actual_values.values[t]), 2)\n","                obs_variance += np.power((mean_actual - actual_values.values[t]), 2)\n","\n","            nse_values[var] = np.round(1 - (pred_variance / obs_variance), 4)\n","        return nse_values\n","\n","    df = bucket_dictionary[ibuc]\n","\n","    # Get predictions from both models\n","    original_predictions = __make_prediction(lstm_original, \"original\")\n","    persistent_predictions = __make_prediction(lstm_persistent, \"persistent\")\n","\n","    # Compute NSE for both models\n","    original_nse = __compute_nse(original_predictions)\n","    persistent_nse = __compute_nse(persistent_predictions)\n","\n","    print(f\"\\n=== COMPARISON FOR BUCKET {ibuc} ===\")\n","    print(\"Nash-Sutcliffe Efficiency (NSE):\")\n","    for var in output_vars:\n","        print(f\"  {var}:\")\n","        print(f\"    Original:    {original_nse[var]}\")\n","        print(f\"    Persistent:  {persistent_nse[var]}\")\n","        print(f\"    Difference:  {persistent_nse[var] - original_nse[var]:.4f}\")\n","\n","    # Plot comparisons\n","    fig, axes = plt.subplots(len(output_vars), 1, figsize=(15, 5*len(output_vars)))\n","    if len(output_vars) == 1:\n","        axes = [axes]\n","\n","    for ax, var in zip(axes, output_vars):\n","        obs_start = val_start + seq_length\n","        obs_end = obs_start + n_plot - 1\n","\n","        actual_values = df.loc[obs_start:obs_end, var].values\n","        ax.plot(actual_values, 'k-', label='Actual', linewidth=2, alpha=0.8)\n","        ax.plot(original_predictions[var][:n_plot], 'r--', label='Original LSTM', alpha=0.8)\n","        ax.plot(persistent_predictions[var][:n_plot], 'b--', label='Persistent LSTM', alpha=0.8)\n","\n","        ax.set_title(f'{var} - Prediction Comparison\\n'\n","                    f'Original NSE: {original_nse[var]}, Persistent NSE: {persistent_nse[var]}')\n","        ax.set_xlabel('Time Step')\n","        ax.set_ylabel(var)\n","        ax.legend()\n","        ax.grid(True, alpha=0.3)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return original_nse, persistent_nse\n","\n","def compare_learning_curves(original_results, persistent_results, buckets_for_training):\n","    \"\"\"Compare learning curves of both approaches\"\"\"\n","    # Original model metrics\n","    orig_losses = np.stack([original_results[b]['loss'] for b in buckets_for_training])\n","    orig_rmses = np.stack([original_results[b]['RMSE'] for b in buckets_for_training])\n","\n","    # Persistent model metrics\n","    persist_losses = np.stack([persistent_results[b]['loss'] for b in buckets_for_training])\n","    persist_rmses = np.stack([persistent_results[b]['RMSE'] for b in buckets_for_training])\n","\n","    epochs = np.arange(orig_losses.shape[1])\n","\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n","\n","    # Loss comparison\n","    ax1.plot(epochs, orig_losses.mean(axis=0), 'r-', label='Original LSTM', linewidth=2)\n","    ax1.plot(epochs, persist_losses.mean(axis=0), 'b-', label='Persistent LSTM', linewidth=2)\n","    ax1.fill_between(epochs,\n","                    np.percentile(orig_losses, 25, axis=0),\n","                    np.percentile(orig_losses, 75, axis=0),\n","                    color='red', alpha=0.2)\n","    ax1.fill_between(epochs,\n","                    np.percentile(persist_losses, 25, axis=0),\n","                    np.percentile(persist_losses, 75, axis=0),\n","                    color='blue', alpha=0.2)\n","    ax1.set_xlabel('Epoch')\n","    ax1.set_ylabel('Loss')\n","    ax1.set_title('Training Loss Comparison')\n","    ax1.legend()\n","    ax1.grid(True, alpha=0.3)\n","\n","    # RMSE comparison\n","    ax2.plot(epochs, orig_rmses.mean(axis=0), 'r-', label='Original LSTM', linewidth=2)\n","    ax2.plot(epochs, persist_rmses.mean(axis=0), 'b-', label='Persistent LSTM', linewidth=2)\n","    ax2.fill_between(epochs,\n","                    np.percentile(orig_rmses, 25, axis=0),\n","                    np.percentile(orig_rmses, 75, axis=0),\n","                    color='red', alpha=0.2)\n","    ax2.fill_between(epochs,\n","                    np.percentile(persist_rmses, 25, axis=0),\n","                    np.percentile(persist_rmses, 75, axis=0),\n","                    color='blue', alpha=0.2)\n","    ax2.set_xlabel('Epoch')\n","    ax2.set_ylabel('RMSE')\n","    ax2.set_title('Training RMSE Comparison')\n","    ax2.legend()\n","    ax2.grid(True, alpha=0.3)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Print final performance comparison\n","    print(\"\\n=== FINAL PERFORMANCE COMPARISON ===\")\n","    print(f\"Original LSTM - Final Loss: {orig_losses.mean(axis=0)[-1]:.4f}, Final RMSE: {orig_rmses.mean(axis=0)[-1]:.4f}\")\n","    print(f\"Persistent LSTM - Final Loss: {persist_losses.mean(axis=0)[-1]:.4f}, Final RMSE: {persist_rmses.mean(axis=0)[-1]:.4f}\")\n","\n","    loss_improvement = ((orig_losses.mean(axis=0)[-1] - persist_losses.mean(axis=0)[-1]) / orig_losses.mean(axis=0)[-1]) * 100\n","    rmse_improvement = ((orig_rmses.mean(axis=0)[-1] - persist_rmses.mean(axis=0)[-1]) / orig_rmses.mean(axis=0)[-1]) * 100\n","\n","    print(f\"Loss Improvement: {loss_improvement:+.2f}%\")\n","    print(f\"RMSE Improvement: {rmse_improvement:+.2f}%\")\n","\n","# =========================================\n","# MAIN EXECUTION WITH BOTH APPROACHES\n","# =========================================\n","torch.manual_seed(1)\n","\n","# Initialize both models\n","lstm_original = LSTM1(num_classes=n_output,\n","                     input_size=n_input,\n","                     hidden_size=hidden_state_size,\n","                     num_layers=num_layers,\n","                     batch_size=batch_size,\n","                     seq_length=seq_length).to(device=device)\n","\n","lstm_persistent = PersistentLSTM(num_classes=n_output,\n","                               input_size=n_input,\n","                               hidden_size=hidden_state_size,\n","                               num_layers=num_layers,\n","                               batch_size=batch_size,\n","                               seq_length=seq_length).to(device=device)\n","\n","# Fit scalers\n","def fit_scaler():\n","    frames = [bucket_dictionary[ibuc].loc[train_start:train_end, input_vars] for ibuc in buckets_for_training]\n","    df_in = pd.concat(frames)\n","    scaler_in = StandardScaler()\n","    _ = scaler_in.fit_transform(df_in)\n","\n","    frames = [bucket_dictionary[ibuc].loc[train_start:train_end, output_vars] for ibuc in buckets_for_training]\n","    df_out = pd.concat(frames)\n","    scaler_out = StandardScaler()\n","    _ = scaler_out.fit_transform(df_out)\n","    return scaler_in, scaler_out\n","\n","scaler_in, scaler_out = fit_scaler()\n","\n","# Create data loaders\n","k_preds = 1\n","\n","def make_data_loader(start, end, bucket_list):\n","    loader = {}\n","    np_seq_X = {}\n","    np_seq_y = {}\n","\n","    for ibuc in bucket_list:\n","        df = bucket_dictionary[ibuc]\n","        # scale inputs and outputs\n","        Xin = scaler_in.transform(df.loc[start:end, input_vars])\n","        Yin = scaler_out.transform(df.loc[start:end, output_vars])\n","\n","        # number of samples: full window length = seq_length + k_preds\n","        n_total = Xin.shape[0]\n","        n_samples = n_total - seq_length + 1\n","\n","        # allocate arrays: inputs always seq_length, outputs now seq_length as before\n","        X = np.zeros((n_samples, seq_length, n_input))\n","        Y = np.zeros((n_samples, seq_length, n_output))\n","\n","        for i in range(n_samples):\n","            t0 = i + seq_length\n","            X[i] = Xin[i:t0]\n","            Y[i] = Yin[i:t0]\n","\n","        np_seq_X[ibuc] = X\n","        np_seq_y[ibuc] = Y\n","\n","        ds = torch.utils.data.TensorDataset(\n","            torch.Tensor(X),\n","            torch.Tensor(Y)\n","        )\n","        loader[ibuc] = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True)\n","\n","    return loader, np_seq_X, np_seq_y\n","\n","train_loader, np_train_seq_X, np_train_seq_y = make_data_loader(train_start, train_end, buckets_for_training)\n","val_loader, np_val_seq_X, np_val_seq_y = make_data_loader(val_start, val_end, buckets_for_val)\n","test_loader, np_test_seq_X, np_test_seq_y = make_data_loader(test_start, test_end, buckets_for_test)\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"TRAINING ORIGINAL LSTM MODEL\")\n","print(\"=\"*60)\n","lstm_original, original_results = train_original_model(lstm_original, train_loader, buckets_for_training)\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"TRAINING PERSISTENT LSTM MODEL\")\n","print(\"=\"*60)\n","lstm_persistent, persistent_results = train_persistent_model(lstm_persistent, train_loader, buckets_for_training)\n","\n","# Compare learning curves\n","print(\"\\n\" + \"=\"*60)\n","print(\"COMPARING LEARNING CURVES\")\n","print(\"=\"*60)\n","compare_learning_curves(original_results, persistent_results, buckets_for_training)\n","\n","# Compare validation performance\n","print(\"\\n\" + \"=\"*60)\n","print(\"COMPARING VALIDATION PERFORMANCE\")\n","print(\"=\"*60)\n","all_original_nse = []\n","all_persistent_nse = []\n","\n","for ibuc in buckets_for_val[:3]:  # Compare on first 3 validation buckets\n","    original_nse, persistent_nse = check_validation_period_comparison(\n","        lstm_original, lstm_persistent, np_val_seq_X, ibuc\n","    )\n","    all_original_nse.append(original_nse)\n","    all_persistent_nse.append(persistent_nse)\n","\n","# Final summary\n","print(\"\\n\" + \"=\"*60)\n","print(\"FINAL SUMMARY\")\n","print(\"=\"*60)\n","print(\"Average NSE across validation buckets:\")\n","for var in output_vars:\n","    orig_avg = np.mean([nse[var] for nse in all_original_nse])\n","    persist_avg = np.mean([nse[var] for nse in all_persistent_nse])\n","    improvement = persist_avg - orig_avg\n","    print(f\"{var}:\")\n","    print(f\"  Original:    {orig_avg:.4f}\")\n","    print(f\"  Persistent:  {persist_avg:.4f}\")\n","    print(f\"  Improvement: {improvement:+.4f} ({improvement/orig_avg*100:+.1f}%)\")"]},{"cell_type":"code","metadata":{"id":"3979eab4"},"source":["!pip install pyflo"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":581},"id":"e36b4b9f","executionInfo":{"status":"ok","timestamp":1761010951224,"user_tz":300,"elapsed":13803,"user":{"displayName":"Majid Hussain Shah 951-FBAS/MSMA/F23","userId":"05397425731334918871"}},"outputId":"684d7ce8-6b6d-4b41-ebc0-6e1591958dfc"},"source":["!pip install pyflo"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyflo\n","  Downloading pyflo-0.3.3.tar.gz (56 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from pyflo) (0.12.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from pyflo) (3.10.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pyflo) (2.0.2)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from pyflo) (3.2.5)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from pyflo) (2.9.0.post0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from pyflo) (2025.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pyflo) (1.16.2)\n","Collecting simpleeval (from pyflo)\n","  Downloading simpleeval-1.0.3-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from pyflo) (1.17.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyflo) (1.3.3)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyflo) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyflo) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyflo) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyflo) (11.3.0)\n","Downloading simpleeval-1.0.3-py3-none-any.whl (15 kB)\n","Building wheels for collected packages: pyflo\n","  Building wheel for pyflo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyflo: filename=pyflo-0.3.3-py3-none-any.whl size=72000 sha256=a60f92572fe5c82aaf64b2f80257fce92d1d176fc268c39912662b454f5a0881\n","  Stored in directory: /root/.cache/pip/wheels/c8/84/50/03b761d910bd63f2db47288e50729c9bf30b12f870b297dcda\n","Successfully built pyflo\n","Installing collected packages: simpleeval, pyflo\n","Successfully installed pyflo-0.3.3 simpleeval-1.0.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["site"]},"id":"f8b90fb261da4830ba8baa8266705a90"}},"metadata":{}}]}]}